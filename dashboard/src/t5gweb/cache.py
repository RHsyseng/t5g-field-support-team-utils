"""cache.py: caching functions for the t5gweb"""

import datetime
import json
import logging
import re
import time
import xmlrpc

import bugzilla
import requests
from jira.exceptions import JIRAError
from t5gweb import libtelco5g
from t5gweb.database import load_cases_postgres, load_jira_card_postgres
from t5gweb.database.operations import get_current_sprint_cards_from_postgres
from t5gweb.utils import format_comment, format_date, make_headers


def get_cases(cfg):
    # https://source.redhat.com/groups/public/hydra/hydra_integration_platform_cee_integration_wiki/hydras_api_layer

    token = libtelco5g.get_token(cfg["offline_token"])
    query = cfg["query"]
    fields = ",".join(cfg["fields"])
    query = f"({query})"
    num_cases = cfg["max_portal_results"]
    payload = {"q": query, "partnerSearch": "false", "rows": num_cases, "fl": fields}
    headers = make_headers(token)
    url = f"{cfg['redhat_api']}/search/cases"

    logging.warning("searching the portal for cases")
    start = time.time()
    r = requests.get(url, headers=headers, params=payload)
    r.raise_for_status()
    cases_json = r.json()["response"]["docs"]
    end = time.time()
    logging.warning("found %s cases in %s seconds", len(cases_json), end - start)
    cases = {}
    for case in cases_json:
        cases[case["case_number"]] = {
            "owner": case["case_owner"],
            "severity": case["case_severity"],
            "account": case["case_account_name"],
            "problem": case["case_summary"],
            "status": case["case_status"],
            "createdate": case["case_createdDate"],
            "last_update": case["case_lastModifiedDate"],
            "description": case["case_description"],
            "product": case["case_product"][0] + " " + case["case_version"],
            "product_version": case["case_version"],
        }
        # Sometimes there is no BZ attached to the case
        if "case_bugzillaNumber" in case:
            cases[case["case_number"]]["bug"] = case["case_bugzillaNumber"]
        # Sometimes there is no tag attached to the case
        if "case_tags" in case:
            case_tags = case["case_tags"]
            if len(case_tags) == 1:
                tags = case_tags[0].split(";")  # csv instead of a proper list
            else:
                tags = case_tags
            cases[case["case_number"]]["tags"] = tags
        if "case_closedDate" in case:
            cases[case["case_number"]]["closeddate"] = case["case_closedDate"]

    # Load to Postgres first, then update Redis cache
    postgres_success = False
    try:
        logging.warning("Loading cases to Postgres database")
        load_cases_postgres(cases)
        logging.warning("Successfully loaded cases to Postgres")
        postgres_success = True
    except Exception as e:
        logging.error("Failed to load cases to Postgres: %s. Skipping Redis cache update.", e)
        # Don't update Redis if Postgres load fails to maintain data consistency
        return

    # # Only update Redis if Postgres succeeded
    # if postgres_success:
    #     try:
    #         logging.warning("Updating Redis cache with cases")
    #         libtelco5g.redis_set("cases", json.dumps(cases))
    #         logging.warning("Successfully updated Redis cache with cases")
    #     except Exception as e:
    #         logging.error("Failed to update Redis cache with cases: %s. Data is safely stored in Postgres.", e)
    #         # Postgres data is safe, Redis cache failure is not critical


def get_escalations(cfg, cases):
    """Get cases that have been escalated by querying the escalations JIRA board

    Args:
        cfg: generated by utils.set_cfg()
        cases: cases returned from portal API using the configured query

    Returns:
        list: open Jira cards that have been escalated
    """
    if (
        cases is None
        or cfg["jira_escalations_project"] is None
        or cfg["jira_escalations_label"] is None
    ):
        return None

    logging.warning("getting escalated cases from JIRA")
    jira_conn = libtelco5g.jira_connection(cfg)
    max_cards = cfg["max_jira_results"]
    project = libtelco5g.get_project_id(jira_conn, cfg["jira_escalations_project"])
    escalations_label = cfg["jira_escalations_label"]
    jira_query = (
        f"project = {project.id} AND labels = "
        f'"{escalations_label}" AND status != "Closed"'
    )

    escalated_cards = jira_conn.search_issues(jira_query, 0, max_cards).iterable
    escalations = []
    for card in escalated_cards:
        issue = jira_conn.issue(card)
        case = issue.fields.customfield_12313441  # SDFC Case Links in escalations proj.
        if case is not None:
            escalations.append(case)
    return escalations


def get_cards(cfg, self=None, background=False):
    """Pull the latest information from the JIRA cards"""

    # Get cached data
    cached_data = _get_cached_data()
    cases, bugs, issues, escalations, details = cached_data

    # Get JIRA connection and card list
    jira_conn = libtelco5g.jira_connection(cfg)
    card_list = _get_jira_cards_list(cfg, jira_conn)

    # Process each card
    jira_cards = {}
    time_now = datetime.datetime.now(datetime.timezone.utc)

    for index, card in enumerate(card_list):
        if background:
            _update_progress(self, index, len(card_list))

        try:
            card_data = _build_card_data(
                card,
                jira_conn,
                cases,
                bugs,
                issues,
                escalations,
                details,
                time_now,
                cfg,
            )
            if card_data:
                jira_cards[card.key] = card_data
                # Load to Postgres
                try:
                    load_jira_card_postgres(cases, card_data["case_number"], card)
                except Exception as postgres_error:
                    logging.error("Failed to load card %s to Postgres: %s", card.key, postgres_error)
                    # Remove from jira_cards if Postgres load fails to maintain consistency
                    jira_cards.pop(card.key, None)
                    continue

        except Exception as e:
            logging.warning("Error processing card %s: %s", card, str(e))
            continue


    # After successful card processing, refresh cache from Postgres to get current sprint data
    try:
        logging.warning("Refreshing Redis cache from Postgres to get current sprint data")
        refresh_result = refresh_cache_from_postgres(cfg)
        logging.warning("Cache refresh result: %s", refresh_result)
    except Exception as e:
        logging.error("Failed to refresh cache from Postgres: %s", e)

    return {"cards cached": len(jira_cards)}


def _get_cached_data():
    # Generated by: Cursor
    """Get all cached data needed for card processing"""
    cases = libtelco5g.redis_get("cases")
    bugs = libtelco5g.redis_get("bugs")
    issues = libtelco5g.redis_get("issues")
    escalations = libtelco5g.redis_get("escalations")
    details = libtelco5g.redis_get("details")
    return cases, bugs, issues, escalations, details


def _execute_jira_query_with_retry(jira_conn, jira_query, cfg, max_results):
    # Generated by: Cursor
    """Execute JIRA query with automatic retry on authentication errors"""
    try:
        return jira_conn.search_issues(jira_query, 0, max_results).iterable
    except JIRAError:
        logging.warning("JIRA Exception. Possible 401. Reconnecting.....")
        jira_conn = libtelco5g.jira_connection(cfg)
        return jira_conn.search_issues(jira_query, 0, max_results).iterable


def _get_jira_issue_with_retry(jira_conn, issue_key, cfg):
    # Generated by: Cursor
    """Get JIRA issue with automatic retry on authentication errors"""
    try:
        return jira_conn.issue(issue_key)
    except JIRAError:
        logging.warning("JIRA Exception. Possible 401. Reconnecting.....")
        jira_conn = libtelco5g.jira_connection(cfg)
        return jira_conn.issue(issue_key)


def _get_jira_cards_list(cfg, jira_conn):
    # Generated by: Cursor
    """Get the list of JIRA cards based on configuration"""
    max_cards = cfg["max_jira_results"]
    project = libtelco5g.get_project_id(jira_conn, cfg["project"])
    board = libtelco5g.get_board_id(jira_conn, cfg["board"])

    if cfg["sprintname"] and cfg["sprintname"] != "":
        sprint = libtelco5g.get_latest_sprint(jira_conn, board.id, cfg["sprintname"])
        jira_query = (
            "sprint=" + str(sprint.id) + ' AND labels = "' + cfg["jira_query"] + '"'
        )
        logging.warning("sprint: %s", sprint)
    else:
        jira_query = (
            "project=" + str(project.id) + ' AND labels = "' + cfg["jira_query"] + '"'
        )

    logging.warning("pulling cards from jira")
    card_list = _execute_jira_query_with_retry(jira_conn, jira_query, cfg, max_cards)

    return card_list


def _update_progress(self, current, total):
    # Generated by: Cursor
    """Update task progress for background processing"""
    self.update_state(
        state="PROGRESS",
        meta={
            "current": current,
            "total": total,
            "status": "Refreshing Cards in Background...",
        },
    )


def _build_card_data(
    card, jira_conn, cases, bugs, issues, escalations, details, time_now, cfg
):
    # Generated by: Cursor
    """Build complete card data for a single JIRA card"""
    issue = _get_jira_issue_with_retry(jira_conn, card, cfg)

    # Extract case number from summary
    case_number = issue.fields.summary.split(":")[0]
    if not re.match("[0-9]{8}", case_number):
        logging.warning("error parsing case number for (%s)", card)
        return None

    if not case_number or case_number not in cases.keys():
        logging.warning("card isn't associated with a case. discarding (%s)", card)
        return None

    # Get comments
    comments = _get_card_comments(issue.fields.comment.comments)

    # Get assignee and contributor info
    assignee = _get_assignee_info(issue)
    contributor = _get_contributor_info(issue)

    # Get case-related data
    case_data = cases[case_number]
    tags = case_data.get("tags", [])

    # Get bug info
    bugzilla = _get_bug_info(case_number, case_data, bugs)

    # Get issues info
    case_issues = issues.get(case_number) if issues else None

    # Get escalation info
    escalation_info = _get_escalation_info(
        case_number, escalations, case_issues, issue.fields.labels, cfg
    )

    # Get case details
    case_detail_info = _get_case_detail_info(case_number, details)

    # Get label-based flags
    label_flags = _get_label_flags(issue.fields.labels, escalation_info["escalated"])

    # Build the complete card data
    return {
        "card_status": libtelco5g.status_map[issue.fields.status.name],
        "card_created": issue.fields.created,
        "account": case_data["account"],
        "summary": case_data["problem"],
        "description": case_data["description"],
        "comments": comments,
        "assignee": assignee,
        "contributor": contributor,
        "case_number": case_number,
        "tags": tags,
        "labels": issue.fields.labels,
        "bugzilla": bugzilla,
        "issues": case_issues,
        "severity": re.search(r"[a-zA-Z]+", case_data["severity"]).group(),
        "priority": issue.fields.priority.name,
        "escalated": escalation_info["escalated"],
        "escalated_link": escalation_info["escalated_link"],
        "potential_escalation": label_flags["potential_escalation"],
        "product": case_data["product"],
        "case_status": case_data["status"],
        "crit_sit": case_detail_info["crit_sit"],
        "group_name": case_detail_info["group_name"],
        "case_updated_date": datetime.datetime.strftime(
            format_date(case_data["last_update"]),
            "%Y-%m-%d %H:%M",
        ),
        "case_days_open": (
            time_now.replace(tzinfo=None) - format_date(case_data["createdate"])
        ).days,
        "case_created": case_data["createdate"],
        "notified_users": case_detail_info["notified_users"],
        "relief_at": case_detail_info["relief_at"],
        "resolved_at": case_detail_info["resolved_at"],
        "daily_telco": label_flags["daily_telco"],
    }


def _get_card_comments(comments):
    # Generated by: Cursor
    """Extract and format card comments"""
    card_comments = []
    for comment in comments:
        body = format_comment(comment)
        tstamp = comment.updated
        card_comments.append((body, tstamp))
    return card_comments


def _get_assignee_info(issue):
    # Generated by: Cursor
    """Extract assignee information from JIRA issue"""
    assignee = {"displayName": None, "key": None, "name": None}
    if issue.fields.assignee:
        assignee = {
            "displayName": issue.fields.assignee.displayName,
            "key": issue.fields.assignee.key,
            "name": issue.fields.assignee.name,
        }
    return assignee


def _get_contributor_info(issue):
    # Generated by: Cursor
    """Extract contributor information from JIRA issue"""
    contributor = []
    if issue.fields.customfield_12315950:
        for engineer in issue.fields.customfield_12315950:
            contributor.append(
                {
                    "displayName": engineer.displayName,
                    "key": engineer.key,
                    "name": engineer.name,
                }
            )
    return contributor


def _get_bug_info(case_number, case_data, bugs):
    # Generated by: Cursor
    """Get bugzilla information for the case"""
    if "bug" in case_data.keys() and bugs is not None and case_number in bugs.keys():
        return bugs[case_number]
    return None


def _get_escalation_info(case_number, escalations, case_issues, labels, cfg):
    # Generated by: Cursor
    """Determine escalation status and links"""
    escalated = bool(escalations and case_number in escalations)
    escalated_link = None

    if case_issues:
        for case_issue in case_issues:
            if cfg["jira_escalations_project"] in case_issue["id"]:
                escalated_link = case_issue["url"]
                break

    return {"escalated": escalated, "escalated_link": escalated_link}


def _get_case_detail_info(case_number, details):
    # Generated by: Cursor
    """Get case detail information from cache"""
    if case_number in details.keys():
        return {
            "crit_sit": details[case_number]["crit_sit"],
            "group_name": details[case_number]["group_name"],
            "notified_users": details[case_number]["notified_users"],
            "relief_at": details[case_number]["relief_at"],
            "resolved_at": details[case_number]["resolved_at"],
        }
    else:
        return {
            "crit_sit": False,
            "group_name": None,
            "notified_users": [],
            "relief_at": None,
            "resolved_at": None,
        }


def _get_label_flags(labels, escalated):
    # Generated by: Cursor
    """Extract boolean flags based on JIRA labels"""
    potential_escalation = "PotentialEscalation" in labels and not escalated
    daily_telco = "Daily_Telco_OCP" in labels

    return {"potential_escalation": potential_escalation, "daily_telco": daily_telco}


def get_case_details(cfg):
    """Caches CritSit and CaseGroup from open cases"""
    cases = libtelco5g.redis_get("cases")
    if cases is None:
        libtelco5g.redis_set("details", json.dumps(None))
        libtelco5g.redis_set("case_bz", json.dumps(None))
        return

    bz_dict = {}
    token = libtelco5g.get_token(cfg["offline_token"])
    headers = make_headers(token)
    case_details = {}
    logging.warning("getting all bugzillas and case details")
    for case in cases:
        if cases[case]["status"] != "Closed":
            case_endpoint = f"{cfg['redhat_api']}/v1/cases/{case}"
            r_case = requests.get(case_endpoint, headers=headers)
            if r_case.status_code == 401:
                token = libtelco5g.get_token(cfg["offline_token"])
                headers = make_headers(token)
                r_case = requests.get(case_endpoint, headers=headers)

            crit_sit = r_case.json().get("critSit", False)
            group_name = r_case.json().get("groupName", None)
            notified_users = r_case.json().get("notifiedUsers", [])
            relief_at = r_case.json().get("reliefAt", None)
            resolved_at = r_case.json().get("resolvedAt", None)

            case_details[case] = {
                "crit_sit": crit_sit,
                "group_name": group_name,
                "notified_users": notified_users,
                "relief_at": relief_at,
                "resolved_at": resolved_at,
            }
            if "bug" in cases[case]:
                bz_dict[case] = r_case.json()["bugzillas"]

    libtelco5g.redis_set("details", json.dumps(case_details))
    libtelco5g.redis_set("case_bz", json.dumps(bz_dict))


def get_bz_details(cfg):
    """Get details about Bugzillas from API"""
    logging.warning("getting additional info via bugzilla API")
    bz_dict = libtelco5g.redis_get("case_bz")
    if bz_dict is None or cfg["bz_key"] is None or cfg["bz_key"] == "":
        libtelco5g.redis_set("bugs", json.dumps(None))
        return

    bz_url = "bugzilla.redhat.com"
    bz_api = bugzilla.Bugzilla(bz_url, api_key=cfg["bz_key"])
    for case in bz_dict:
        for bug in bz_dict[case]:
            try:
                bugs = bz_api.getbug(bug["bugzillaNumber"])
            except xmlrpc.client.Fault:
                logging.warning(
                    "error retrieving bug %s - restricted?", bug["bugzillaNumber"]
                )
                bugs = None
            if bugs:
                bug["target_release"] = bugs.target_release
                bug["assignee"] = bugs.assigned_to
                bug["last_change_time"] = datetime.datetime.strftime(
                    datetime.datetime.strptime(
                        str(bugs.last_change_time), "%Y%m%dT%H:%M:%S"
                    ),
                    "%Y-%m-%d",
                )  # convert from xmlrpc.client.DateTime to str and reformat
                bug["internal_whiteboard"] = bugs.internal_whiteboard
                bug["qa_contact"] = bugs.qa_contact
                bug["severity"] = bugs.severity
            else:
                bug["target_release"] = ["unavailable"]
                bug["assignee"] = "unavailable"
                bug["last_change_time"] = "unavailable"
                bug["internal_whiteboard"] = "unavailable"
                bug["qa_contact"] = "unavailable"
                bug["severity"] = "unavailable"

    libtelco5g.redis_set("bugs", json.dumps(bz_dict))


def get_issue_details(cfg):
    """Cache issues associated with cases"""
    logging.warning("caching issues")

    cases = libtelco5g.redis_get("cases")
    if cases is None:
        libtelco5g.redis_set("issues", json.dumps(None))
        return

    # Setup authentication and JIRA connection
    token, headers, jira_conn = _setup_issue_processing(cfg)

    # Process issues for all open cases
    jira_issues = {}
    open_cases = [case for case in cases if cases[case]["status"] != "Closed"]

    for case in open_cases:
        try:
            case_issues = _process_case_issues(case, cfg, token, headers, jira_conn)
            if case_issues:
                jira_issues[case] = case_issues
        except Exception as e:
            logging.warning("Error processing issues for case %s: %s", case, str(e))
            continue

    # Cache the results
    libtelco5g.redis_set("issues", json.dumps(jira_issues))
    logging.warning("issues cached")


def _setup_issue_processing(cfg):
    # Generated by: Cursor
    """Setup authentication and JIRA connection for issue processing"""
    # Reuse the existing libtelco5g setup pattern for consistency
    token = libtelco5g.get_token(cfg["offline_token"])
    headers = make_headers(token)
    jira_conn = libtelco5g.jira_connection(cfg)

    return token, headers, jira_conn


def _process_case_issues(case, cfg, token, headers, jira_conn):
    # Generated by: Cursor
    """Process all issues for a specific case"""
    # Get issues from API
    issues_data = _get_case_issues_from_api(case, cfg, token, headers)
    if not issues_data:
        return None

    case_issues = []
    for issue in issues_data:
        if "title" in issue.keys():
            try:
                processed_issue = _process_single_jira_issue(issue, jira_conn)
                if processed_issue:
                    case_issues.append(processed_issue)
            except JIRAError:
                logging.warning("Can't access %s", issue["resourceKey"])
                continue
            except Exception as e:
                logging.warning(
                    "Error processing issue %s: %s",
                    issue.get("resourceKey", "unknown"),
                    str(e),
                )
                continue

    return case_issues if case_issues else None


def _get_case_issues_from_api(case, cfg, token, headers):
    # Generated by: Cursor
    """Get issues for a case from the Red Hat API"""
    issues_url = f"{cfg['redhat_api']}/cases/{case}/jiras"
    issues = requests.get(issues_url, headers=headers)

    # Handle 401 authorization errors
    if issues.status_code == 401:
        token = libtelco5g.get_token(cfg["offline_token"])
        headers = make_headers(token)
        issues = requests.get(issues_url, headers=headers)

    if issues.status_code == 200 and len(issues.json()) > 0:
        return issues.json()

    return None


def _process_single_jira_issue(issue, jira_conn):
    # Generated by: Cursor
    """Process a single JIRA issue and extract all relevant fields"""
    try:
        bug = jira_conn.issue(issue["resourceKey"])
    except JIRAError:
        logging.warning("Can't access %s", issue["resourceKey"])
        return None

    # Extract all JIRA fields
    jira_fields = _extract_jira_fields(bug)

    # Build the complete issue data
    return {
        "id": issue["resourceKey"],
        "url": issue["resourceURL"],
        "title": issue["title"],
        "status": issue["status"],
        "updated": datetime.datetime.strftime(
            format_date(str(issue["lastModifiedDate"])),
            "%Y-%m-%d",
        ),
        **jira_fields,
    }


def _extract_jira_fields(bug):
    # Generated by: Cursor
    """Extract all relevant fields from a JIRA bug"""
    fields = {}

    # QA contact
    fields["qa_contact"] = _extract_qa_contact(bug)

    # Severity
    fields["jira_severity"] = _extract_jira_severity(bug)

    # Issue type
    fields["jira_type"] = _extract_jira_type(bug)

    # Assignee
    fields["assignee"] = _extract_assignee_email(bug)

    # Fix versions (target release)
    fields["fix_versions"] = _extract_fix_versions(bug)

    # Priority
    fields["priority"] = _extract_priority(bug)

    # Private keywords
    fields["private_keywords"] = _extract_private_keywords(bug)

    return fields


def _extract_qa_contact(bug):
    # Generated by: Cursor
    """Extract QA contact from JIRA bug"""
    try:
        return bug.fields.customfield_12315948.emailAddress
    except AttributeError:
        return None


def _extract_jira_severity(bug):
    # Generated by: Cursor
    """Extract severity from JIRA bug"""
    try:
        return bug.fields.customfield_12316142.value
    except AttributeError:
        return None


def _extract_jira_type(bug):
    # Generated by: Cursor
    """Extract issue type from JIRA bug"""
    try:
        return bug.fields.issuetype.name
    except AttributeError:
        return None


def _extract_assignee_email(bug):
    # Generated by: Cursor
    """Extract assignee email from JIRA bug"""
    if bug.fields.assignee is not None:
        return bug.fields.assignee.emailAddress
    return None


def _extract_fix_versions(bug):
    # Generated by: Cursor
    """Extract fix versions from JIRA bug"""
    if len(bug.fields.fixVersions) > 0:
        return [version.name for version in bug.fields.fixVersions]
    return None


def _extract_priority(bug):
    # Generated by: Cursor
    """Extract priority from JIRA bug"""
    if bug.fields.priority:
        return bug.fields.priority.name
    return None


def _extract_private_keywords(bug):
    # Generated by: Cursor
    """Extract private keywords from JIRA bug"""
    try:
        private_keywords_raw = bug.fields.customfield_12323649
    except AttributeError:
        return None

    if private_keywords_raw is not None and len(private_keywords_raw) > 0:
        return [private_keyword.value for private_keyword in private_keywords_raw]
    return None


def refresh_cache_from_postgres(cfg):
    """Refresh Redis cache using data from Postgres database

    This decouples cache updates from data loading - cache is refreshed
    from the database rather than from API calls.
    """
    logging.warning("Refreshing Redis cache from Postgres database")

    try:
        # Get current sprint data from Postgres
        sprint_data = get_current_sprint_cards_from_postgres(cfg)
        cards = sprint_data["cards"]
        cases = sprint_data["cases"]

        # Update Redis cache with Postgres data
        try:
            logging.warning("Updating Redis cache with %d cards from Postgres", len(cards))
            libtelco5g.redis_set("cards", json.dumps(cards))

            logging.warning("Updating Redis cache with %d cases from Postgres", len(cases))
            libtelco5g.redis_set("cases", json.dumps(cases))

            # Update timestamp
            libtelco5g.redis_set(
                "timestamp", json.dumps(str(datetime.datetime.now(datetime.timezone.utc)))
            )

            logging.warning("Successfully refreshed Redis cache from Postgres")
            return {"cards_cached": len(cards), "cases_cached": len(cases)}

        except Exception as redis_error:
            logging.error("Failed to update Redis cache: %s. Data is available in Postgres.", redis_error)
            return {"error": "Redis update failed", "cards_available": len(cards), "cases_available": len(cases)}

    except Exception as postgres_error:
        logging.error("Failed to read data from Postgres: %s", postgres_error)
        return {"error": "Postgres read failed"}


def get_stats():
    logging.warning("caching {} stats")
    all_stats = libtelco5g.redis_get("stats")
    new_stats = libtelco5g.generate_stats()
    tstamp = datetime.datetime.now(datetime.timezone.utc)
    today = tstamp.strftime("%Y-%m-%d")
    stats = {today: new_stats}
    all_stats.update(stats)
    libtelco5g.redis_set("stats", json.dumps(all_stats))
